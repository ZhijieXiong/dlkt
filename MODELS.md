[TOC]

# KT Models

## DKT

[*Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J. Guibas, Jascha Sohl-Dickstein*. **Deep Knowledge Tracing**. Advances in Neural Information Processing Systems 28 (NIPS 2015)](https://proceedings.neurips.cc/paper_files/paper/2015/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html)

Abstract: Knowledge tracing—where a machine models the knowledge of a student as they interact with coursework—is a well established problem in computer supported education. Though effectively modeling student knowledge would have high ed- ucational impact, the task has many inherent challenges. In this paper we explore the utility of using Recurrent Neural Networks (RNNs) to model student learning. The RNN family of models have important advantages over previous methods in that they do not require the explicit encoding of human domain knowledge, and can capture more complex representations of student knowledge. Using neu- ral networks results in substantial improvements in prediction performance on a range of knowledge tracing datasets. Moreover the learned model can be used for intelligent curriculum design and allows straightforward interpretation and dis- covery of structure in student tasks. These results suggest a promising new line of research for knowledge tracing and an exemplary application task for RNNs.

- Improvements in code implementation
  - You can choose to use **concepts or question** as input
  - For datasets with **multiple concepts**, you can choose to **extend a question with multi-concepts into multiple single-concept** refer [here](DOC.md) for details), and then train and evaluate the model. You can also choose to **train and evaluate the model directly at the question level**.

## DKVMN

[*Jiani Zhang, Xingjian Shi, Irwin King, and Dit-Yan Yeung*. **Dynamic Key-Value Memory Networks for Knowledge Tracing**. In Proceedings of the 26th International Conference on World Wide Web (WWW '17)](https://dl.acm.org/doi/abs/10.1145/3038912.3052580)

Abstract: Knowledge Tracing (KT) is a task of tracing evolving knowl- edge state of students with respect to one or more con- cepts as they engage in a sequence of learning activities. One important purpose of KT is to personalize the prac- tice sequence to help students learn knowledge concepts effi- ciently. However, existing methods such as Bayesian Knowl- edge Tracing and Deep Knowledge Tracing either model knowledge state for each predefined concept separately or fail to pinpoint exactly which concepts a student is good at or unfamiliar with. To solve these problems, this work intro- duces a new model called Dynamic Key-Value Memory Net- works (DKVMN) that can exploit the relationships between underlying concepts and directly output a student’s mastery level of each concept. Unlike standard memory-augmented neural networks that facilitate a single memory matrix or two static memory matrices, our model has one static ma- trix called key, which stores the knowledge concepts and the other dynamic matrix called value, which stores and updates the mastery levels of corresponding concepts. Experiments show that our model consistently outperforms the state-of- the-art model in a range of KT datasets. Moreover, the DKVMN model can automatically discover underlying con- cepts of exercises typically performed by human annotations and depict the changing knowledge state of a student.

## SAKT

[*Shalini Pandey, George Karypis*. **A Self-Attentive model for Knowledge Tracing**. EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining](https://experts.umn.edu/en/publications/a-self-attentive-model-for-knowledge-tracing)

Abstract: Knowledge tracing is the task of modeling each student’s mastery of knowledge concepts (KCs) as (s)he engages with a sequence of learning activities. Each student’s knowledge is modeled by estimating the performance of the student on the learning activities. It is an important research area for providing a personalized learning platform to students. In recent years, methods based on Recurrent Neural Net- works (RNN) such as Deep Knowledge Tracing (DKT) and Dynamic Key-Value Memory Network (DKVMN) outper- formed all the traditional methods because of their ability to capture a complex representation of human learning. How- ever, these methods face the issue of not generalizing well while dealing with sparse data which is the case with real- world data as students interact with few KCs. In order to address this issue, we develop an approach that identifies the KCs from the student’s past activities that are rele- vant to the given KC and predicts his/her mastery based on the relatively few KCs that it picked. Since predictions are made based on relatively few past activities, it handles the data sparsity problem better than the methods based on RNN. For identifying the relevance between the KCs, we propose a self-attention based approach, Self Attentive Knowledge Tracing (SAKT). Extensive experimentation on a variety of real-world dataset shows that our model out- performs the state-of-the-art models for knowledge tracing, improving AUC by 4.43% on average.

## qDKT

[*Sonkar, Shashank, et al*. **qdkt: Question-centric deep knowledge tracing**. arXiv preprint arXiv:2005.12442 (2020)](https://arxiv.org/pdf/2005.12442.pdf)

Abstract: Knowledge tracing (KT) models, e.g., the deep knowledge tracing (DKT) model, track an individual learner’s acquisition of skills over time by examining the learner’s performance on questions related to those skills. A practical limitation in most existing KT models is that all questions nested under a particular skill are treated as equivalent observations of a learner’s ability, which is an inaccurate assumption in real-world educational scenarios. To overcome this limitation we introduce qDKT, a variant of DKT that models every learner’s success probability on individual questions over time. First, qDKT incorporates graph Laplacian regularization to smooth predictions under each skill, which is particularly useful when the number of questions in the dataset is big. Second, qDKT uses an initialization scheme inspired by the fastText algorithm, which has found success in a variety of language modeling tasks. Our experiments on several real-world datasets show that qDKT achieves state-of-art performance on predicting learner outcomes. Because of this, qDKT can serve as a simple, yet tough-to-beat, baseline for new question-centric KT models.

- Improvements in code implementation
  - Just like DKT, for datasets with **multiple concepts**, you can choose to **extend a question with multi-concepts into multiple single-concept** refer [here](DOC.md) for details), and then train and evaluate the model. You can also choose to **train and evaluate the model directly at the question level**.

## AKT

[*Aritra Ghosh, Neil Heffernan, and Andrew S. Lan*. **Context-Aware Attentive Knowledge Tracing**. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20)](https://doi.org/10.1145/3394486.3403282)

Abstract: Knowledge tracing (KT) refers to the problem of predicting future learner performance given their past performance in educational applications. Recent developments in KT using flexible deep neural network-based models excel at this task. However, these models of- ten offer limited interpretability, thus making them insufficient for personalized learning, which requires using interpretable feedback and actionable recommendations to help learners achieve better learning outcomes. In this paper, we propose attentive knowledge tracing (AKT), which couples flexible attention-based neural net- work models with a series of novel, interpretable model compo- nents inspired by cognitive and psychometric models. AKT uses a novel monotonic attention mechanism that relates a learner’s future responses to assessment questions to their past responses; attention weights are computed using exponential decay and a context-aware relative distance measure, in addition to the sim- ilarity between questions. Moreover, we use the Rasch model to regularize the concept and question embeddings; these embeddings are able to capture individual differences among questions on the same concept without using an excessive number of parameters. We conduct experiments on several real-world benchmark datasets and show that AKT outperforms existing KT methods (by up to 6% in AUC in some cases) on predicting future learner responses. We also conduct several case studies and show that AKT exhibits excel- lent interpretability and thus has potential for automated feedback and personalization in real-world educational settings.

- Improvements in code implementation: For **datasets with multiple-concepts**, the extended functions are the same as qDKT.

## LPKT

[*Shuanghong Shen, Qi Liu, Enhong Chen, Zhenya Huang, Wei Huang, Yu Yin, Yu Su, and Shijin Wang*. **Learning Process-consistent Knowledge Tracing**. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (KDD '21)](https://dl.acm.org/doi/abs/10.1145/3447548.3467237)

Abstract: Knowledge tracing (KT), which aims to trace students’ changing knowledge state during their learning process, has improved stu- dents’ learning efficiency in online learning systems. Recently, KT has attracted much research attention due to its critical signifi- cance in education. However, most of the existing KT methods pursue high accuracy of student performance prediction but ne- glect the consistency of students’ changing knowledge state with their learning process. In this paper, we explore a new paradigm for the KT task and propose a novel model named Learning Process- consistent Knowledge Tracing (LPKT), which monitors students’ knowledge state through directly modeling their learning process. Specifically, we first formalize the basic learning cell as the tuple exercise—answer time—answer. Then, we deeply measure the learn- ing gain as well as its diversity from the difference of the present and previous learning cells, their interval time, and students’ re- lated knowledge state. We also design a learning gate to distinguish students’ absorptive capacity of knowledge. Besides, we design a forgetting gate to model the decline of students’ knowledge over time, which is based on their previous knowledge state, present learning gains, and the interval time. Extensive experimental re- sults on three public datasets demonstrate that LPKT could obtain more reasonable knowledge state in line with the learning process. Moreover, LPKT also outperforms state-of-the-art KT methods on student performance prediction. Our work indicates a potential fu- ture research direction for KT, which is of both high interpretability and accuracy.

## ATKT

[*Xiaopeng Guo, Zhijie Huang, Jie Gao, Mingyu Shang, Maojing Shu, and Jun Sun.* **Enhancing Knowledge Tracing via Adversarial Training**. In Proceedings of the 29th ACM International Conference on Multimedia (MM '21)](https://dl.acm.org/doi/abs/10.1145/3474085.3475554)

Abstract: We study the problem of knowledge tracing (KT) where the goal is to trace the students’ knowledge mastery over time so as to make predictions on their future performance. Owing to the good representation capacity of deep neural networks (DNNs), recent advances on KT have increasingly concentrated on exploring DNNs to improve the performance of KT. However, we empirically reveal that the DNNs based KT models may run the risk of overfitting, especially on small datasets, leading to limited generalization. In this paper, by leveraging the current advances in adversarial train- ing (AT), we propose an efficient AT based KT method (ATKT) to enhance KT model’s generalization and thus push the limit of KT. Specifically, we first construct adversarial perturbations and add them on the original interaction embeddings as adversarial examples. The original and adversarial examples are further used to jointly train the KT model, forcing it is not only to be robust to the adversarial examples, but also to enhance the generalization over the original ones. To better implement AT, we then present an efficient attentive-LSTM model as KT backbone, where the key is a proposed knowledge hidden state attention module that adap- tively aggregates information from previous knowledge hidden states while simultaneously highlighting the importance of current knowledge hidden state to make a more accurate prediction. Exten- sive experiments on four public benchmark datasets demonstrate that our ATKT achieves new state-of-the-art performance. Code is available at: https://github.com/xiaopengguo/ATKT.

## DIMKT

[*Shuanghong Shen, Zhenya Huang, Qi Liu, Yu Su, Shijin Wang, and Enhong Chen*. **Assessing Student's Dynamic Knowledge State by Exploring the Question Difficulty Effect**. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '22)](https://dl.acm.org/doi/abs/10.1145/3477495.3531939)

Abstract: Knowledge Tracing (KT), which aims to assess students’ dynamic knowledge states when practicing on various questions, is a fun- damental research task for offering intelligent services in online learning systems. Researchers have devoted significant efforts to developing KT models with impressive performance. However, in existing KT methods, the related question difficulty level, which directly affects students’ knowledge state in learning, has not been effectively explored and employed. In this paper, we focus on explor- ing the question difficulty effect on learning to improve student’s knowledge state assessment and propose the DIfficulty Matching Knowledge Tracing (DIMKT) model. Specifically, we first explic- itly incorporate the difficulty level into the question representa- tion. Then, to establish the relation between students’ knowledge state and the question difficulty level during the practice process, we accordingly design an adaptive sequential neural network in three stages: (1) measuring students’ subjective feelings of the ques- tion difficulty before practice; (2) estimating students’ personalized knowledge acquisition while answering questions of different dif- ficulty levels; (3) updating students’ knowledge state in varying degrees to match the question difficulty level after practice. Finally, we conduct extensive experiments on real-world datasets, and the results demonstrate that DIMKT outperforms state-of-the-art KT models. Moreover, DIMKT shows superior interpretability by ex- ploring the question difficulty effect when making predictions. Our codes are available at https://github.com/shshen-closer/DIMKT.

- Improvements in code implementation
  - In the experimental setting of this paper, low-frequency questions and concepts in the dataset will first be discarded. Then the difficulty of questions and concepts is calculated based on the statistical information of the entire dataset as known information. However, this is an unreasonable setting, because theoretically only the training set can be accessed when training the model. Therefore, when implementing DIMKT, I **only used the training set to calculate the difficulty of questions and concepts**. For **low-frequency and zero-frequency** question and concepts in the training set, there is a separate embedding (embedding is used in this model to indicate difficulty).
  - In addition, for **datasets with multiple-concepts**, the extended functions are the same as qDKT.

## CL4KT

[*Wonsung Lee, Jaeyoon Chun, Youngmin Lee, Kyoungsoo Park, and Sungrae Park*. **Contrastive Learning for Knowledge Tracing**. In Proceedings of the ACM Web Conference 2022 (WWW '22)](https://dl.acm.org/doi/abs/10.1145/3485447.3512105)

Abstract: Knowledge tracing is the task of understanding student’s knowl- edge acquisition processes by estimating whether to solve the next question correctly or not. Most deep learning-based methods tackle this problem by identifying hidden representations of knowledge states from learning histories. However, due to the sparse interac- tions between students and questions, the hidden representations can be easily over-fitted and often fail to capture student’s knowl- edge states accurately. This paper introduces a contrastive learning framework for knowledge tracing that reveals semantically similar or dissimilar examples of a learning history and stimulates to learn their relationships. To deal with the complexity of knowledge ac- quisition during learning, we carefully design the components of contrastive learning, such as architectures, data augmentation meth- ods, and hard negatives, taking into account pedagogical rationales. Our extensive experiments on six benchmarks show statistically significant improvements from the previous methods. Further anal- ysis shows how our methods contribute to improving knowledge tracing performances.

## SimpleKT

[*Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang,Weiqi Luo*. **simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing**. The Eleventh International Conference on Learning Representations. 2022](https://openreview.net/forum?id=9HiGqC9C-KA)

Abstract: Knowledge tracing (KT) is the problem of predicting students’ future performance based on their historical interactions with intelligent tutoring systems. Recently, many works present lots of special methods for applying deep neural networks to KT from different perspectives like model architecture, adversarial augmen- tation and etc., which make the overall algorithm and system become more and more complex. Furthermore, due to the lack of standardized evaluation proto- col (Liu et al., 2022), there is no widely agreed KT baselines and published ex- perimental comparisons become inconsistent and self-contradictory, i.e., the re- ported AUC scores of DKT on ASSISTments2009 range from 0.721 to 0.821 (Minn et al., 2018; Yeung & Yeung, 2018). Therefore, in this paper, we provide a strong but simple baseline method to deal with the KT task named SIMPLEKT. Inspired by the Rasch model in psychometrics, we explicitly model question- specific variations to capture the individual differences among questions cover- ing the same set of knowledge components that are a generalization of terms of concepts or skills needed for learners to accomplish steps in a task or a problem. Furthermore, instead of using sophisticated representations to capture student for- getting behaviors, we use the ordinary dot-product attention function to extract the time-aware information embedded in the student learning interactions. Ex- tensive experiments show that such a simple baseline is able to always rank top 3 in terms of AUC scores and achieve 57 wins, 3 ties and 16 loss against 12 DLKT baseline methods on 7 public datasets of different domains. We believe this work serves as a strong baseline for future KT research. Code is available at https://github.com/pykt-team/pykt-toolkit1.

## DTransformer

[*Yu Yin, Le Dai, Zhenya Huang, Shuanghong Shen, Fei Wang, Qi Liu, Enhong Chen, and Xin Li*. **Tracing Knowledge Instead of Patterns: Stable Knowledge Tracing with Diagnostic Transformer**. In Proceedings of the ACM Web Conference 2023 (WWW '23)](https://dl.acm.org/doi/abs/10.1145/3543507.3583255)

Abstract: Knowledge Tracing (KT) aims at tracing the evolution of the knowl- edge states along the learning process of a learner. It has become a crucial task for online learning systems to model the learning process of their users, and further provide their users a personalized learning guidance. However, recent developments in KT based on deep neural networks mostly focus on increasing the accuracy of predicting the next performance of students. We argue that cur- rent KT modeling, as well as training paradigm, can lead to models tracing patterns of learner’s learning activities, instead of their evolving knowledge states. In this paper, we propose a new archi- tecture, Diagnostic Transformer (DTransformer), along with a new training paradigm, to tackle this challenge. With DTransformer, we build the architecture from question-level to knowledge-level, explicitly diagnosing learner’s knowledge profciency from each question mastery states. We also propose a novel training algo- rithm based on contrastive learning that focuses on maintaining the stability of the knowledge state diagnosis. Through extensive experiments, we will show that with its understanding of knowl- edge state evolution, DTransformer achieves a better performance prediction accuracy and more stable knowledge state tracing results. We will also show that DTransformer is less sensitive to specifc patterns with case study. We open-sourced our code and data at https://github.com/yxonic/DTransformer.

## AT-DKT

[*Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Boyu Gao, Weiqi Luo, and Jian Weng*. **Enhancing Deep Knowledge Tracing with Auxiliary Tasks**. In Proceedings of the ACM Web Conference 2023 (WWW '23)](https://dl.acm.org/doi/abs/10.1145/3543507.3583866)

Abstract: Knowledge tracing (KT) is the problem of predicting students’ future performance based on their historical interactions with intelligent tutoring systems. Recent studies have applied multiple types of deep neural networks to solve the KT problem. However, there are two important factors in real-world educational data that are not well represented. First, most existing works augment input representations with the co-occurrence matrix of questions and knowledge components1 (KCs) but fail to explicitly integrate such intrinsic relations into the final response prediction task. Second, the individualized historical performance of students has not been well captured. In this paper, we proposed AT-DKT to improve the prediction performance of the original deep knowledge tracing model with two auxiliary learning tasks, i.e., question tagging (QT) prediction task and individualized prior knowledge (IK) prediction task. Specifically, the QT task helps learn better question representations by predicting whether questions contain specific KCs. The IK task captures students’ global historical performance by progressively predicting student-level prior knowledge that is hidden in students’ historical learning interactions. We conduct comprehensive experiments on three real-world educational datasets and compare the proposed approach to both deep sequential KT models and non-sequential models. Experimental results show that AT-DKT outperforms all sequential models with more than 0.9% improvements of AUC for all datasets, and is almost the second best compared to non-sequential models. Furthermore, we conduct both ablation studies and quantitative analysis to show the effectiveness of auxiliary tasks and the superior prediction outcomes of AT-DKT. To encourage reproducible research, we make our data and code publicly available at https://github.com/pykt-team/pykt-toolkit.

## QIKT

[*Chen, J., Liu, Z., Huang, S., Liu, Q. and Luo, W*. **Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations**. The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)](https://ojs.aaai.org/index.php/AAAI/article/view/26661)

Abstract: Knowledge tracing (KT) is a crucial technique to predict stu- dents’ future performance by observing their historical learn- ing processes. Due to the powerful representation ability of deep neural networks, remarkable progress has been made by using deep learning techniques to solve the KT prob- lem. The majority of existing approaches rely on the *ho- mogeneous question* assumption that questions have equiv- alent contributions if they share the same set of knowledge components. Unfortunately, this assumption is inaccurate in real-world educational scenarios. Furthermore, it is very chal- lenging to interpret the prediction results from the existing deep learning based KT models. Therefore, in this paper, we present QIKT, a question-centric interpretable KT model to address the above challenges. The proposed QIKT approach explicitly models students’ knowledge state variations at a fine-grained level with question-sensitive cognitive represen- tations that are jointly learned from a question-centric knowl- edge acquisition module and a question-centric problem solv- ing module. Meanwhile, the QIKT utilizes an item response theory based prediction layer to generate interpretable predic- tion results. The proposed QIKT model is evaluated on three public real-world educational datasets. The results demon- strate that our approach is superior on the KT prediction task, and it outperforms a wide range of deep learning based KT models in terms of prediction accuracy with better model interpretability. To encourage reproducible results, we have provided all the datasets and code at https://pykt.org/.

# Other Methods

## ME-ADA

model: `[kt_model]_max_entropy_aug.py`

[*Long Zhao, Ting Liu, Xi Peng, Dimitris Metaxas*. **Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness**. Advances in Neural Information Processing Systems 33 (NeurIPS 2020)](https://proceedings.neurips.cc/paper_files/paper/2020)

Abstract: Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difficult to define heuristics to generate effective fictitious target distributions containing "hard" adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regularization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated "hard" adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically significant margin.

## DuoRec

model: `[kt_model]_duo_cl.py`

[*Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang*. **Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation**. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22)](https://dl.acm.org/doi/abs/10.1145/3488560.3498433)

Abstract: Recent advancements of sequential deep learning models such as Transformer and BERT have significantly facilitated the sequential recommendation. However, according to our study, the distribution of item embeddings generated by these models tends to degener- ate into an anisotropic shape, which may result in high semantic similarities among embeddings. In this paper, both empirical and theoretical investigations of this representation degeneration prob- lem are first provided, based on which a novel recommender model DuoRec is proposed to improve the item embeddings distribution. Specifically, in light of the uniformity property of contrastive learn- ing, a contrastive regularization is designed for DuoRec to reshape the distribution of sequence representations. Given the convention that the recommendation task is performed by measuring the simi- larity between sequence representations and item embeddings in the same space via dot product, the regularization can be implicitly applied to the item embedding distribution. Existing contrastive learning methods mainly rely on data level augmentation for user- item interaction sequences through item cropping, masking, or reordering and can hardly provide semantically consistent augmen- tation samples. In DuoRec, a model-level augmentation is proposed based on Dropout to enable better semantic preserving. Further- more, a novel sampling strategy is developed, where sequences having the same target item are chosen hard positive samples. Ex- tensive experiments conducted on five datasets demonstrate the superior performance of the proposed DuoRec model compared with baseline methods. Visualization results of the learned represen- tations validate that DuoRec can largely alleviate the representation degeneration problem.

## ICL

model: `[kt_model]_cluster_cl.py`

[*Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong*. **Intent Contrastive Learning for Sequential Recommendation**. In Proceedings of the ACM Web Conference 2022 (WWW '22)](https://dl.acm.org/doi/abs/10.1145/3485447.3512090)

Abstract: Users’ interactions with items are driven by various intents (e.g., preparing for holiday gifts, shopping for fishing equipment, etc.). However, users’ underlying intents are often unobserved/latent, making it challenging to leverage such latent intents for Sequential recommendation (SR). To investigate the benefits of latent intents and leverage them effectively for recommendation, we propose Intent Contrastive Learning (ICL), a general learning paradigm that leverages a latent intent variable into SR. The core idea is to learn users’ intent distribution functions from unlabeled user behavior sequences and optimize SR models with contrastive self-supervised learning (SSL) by considering the learnt intents to improve recommendation. Specifically, we introduce a latent variable to represent users’ intents and learn the distribution function of the latent variable via clustering. We propose to leverage the learnt intents into SR models via contrastive SSL, which maximizes the agreement between a view of sequence and its corresponding intent. The training is alternated between intent representation learning and the SR model optimization steps within the generalized expectation-maximization (EM) framework. Fusing user intent information into SR also improves model robustness. Experiments conducted on four real-world datasets demonstrate the superiority of the proposed learning paradigm, which improves performance, and robustness against data sparsity and noisy interaction issues 1.

## CoSeRec

model: `[kt_model]_instance_cl.py`

[*Zhiwei Liu, Yongjun Chen, Jia Li, Philip S. Yu, Julian McAuley, Caiming Xiong*. **Contrastive Self-supervised Sequential Recommendation with Robust Augmentation**. arXiv:2108.06479](https://arxiv.org/abs/2108.06479)

Abstract: Sequential Recommendation describes a set of techniques to model dynamic user behavior in order to predict future interactions in sequential user data. At their core, such approaches model transition probabilities between items in a sequence, whether through Markov chains, recurrent networks, or more recently, Transformers. However both old and new issues remain, including data-sparsity and noisy data; such issues can impair performance, especially in complex, parameter-hungry models. In this paper, we investigate the application of contrastive Self-Supervised Learning (SSL) to sequential recommendation, as a way to alleviate some of these issues. Contrastive SSL constructs augmentations from unlabelled instances, where agreements among positive pairs are maximized. It is challenging to devise a contrastive SSL framework for sequential recommendation, due to its discrete nature, correlations among items, and skewness of length distributions. To this end, we propose a novel framework, Contrastive Self-supervised Learning for Sequential Recommendation (CoSeRec). We introduce two informative augmentation operators leveraging item correlations to create high quality views for contrastive learning. Experimental results on three real-world datasets demonstrate the effectiveness of the proposed method on improving model performance, and the robustness against sparse and noisy data. Our implementation is available: https://github.com/YChen1993/CoSeRec

## MELT

model: `[kt_model]_mutual_enhance4long_tail.py`

[*Kibum Kim, Dongmin Hyun, Sukwon Yun, and Chanyoung Park*. **MELT: Mutual Enhancement of Long-Tailed User and Item for Sequential Recommendation**. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23)](https://dl.acm.org/doi/10.1145/3539618.3591725)

Abstract: The long-tailed problem is a long-standing challenge in Sequential Recommender Systems (SRS) in which the problem exists in terms of both users and items. While many existing studies address the long-tailed problem in SRS, they only focus on either the user or item perspective. However, we discover that the long-tailed user and item problems exist at the same time, and considering only either one of them leads to sub-optimal performance of the other one. In this paper, we propose a novel framework for SRS, called Mutual Enhancement of Long-Tailed user and item (MELT), that jointly alleviates the long-tailed problem in the perspectives of both users and items. MELT consists of bilateral branches each of which is responsible for long-tailed users and items, respectively, and the branches are trained to mutually enhance each other, which is trained effectively by a curriculum learning-based training. MELT is model-agnostic in that it can be seamlessly integrated with existing SRS models. Extensive experiments on eight datasets demonstrate the benefit of alleviating the long-tailed problems in terms of both users and items even without sacrificing the performance of head users and items, which has not been achieved by existing methods. To the best of our knowledge, MELT is the first work that jointly alleviates the long-tailed user and item problems in SRS. Our code is available at https://github.com/rlqja1107/MELT.

## MCLRec

model: `[kt_model]_meta_optimize_cl.py`

[*Xiuyuan Qin, Huanhuan Yuan, Pengpeng Zhao, Junhua Fang, Fuzhen Zhuang, Guanfeng Liu, Yanchi Liu, and Victor Sheng*. **Meta-optimized Contrastive Learning for Sequential Recommendation**. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23)](https://dl.acm.org/doi/abs/10.1145/3539618.3591727)

Abstract: Contrastive Learning (CL) performances as a rising approach to address the challenge of sparse and noisy recommendation data. Although having achieved promising results, most existing CL meth- ods only perform either hand-crafted data or model augmentation for generating contrastive pairs to find a proper augmentation operation for different datasets, which makes the model hard to generalize. Additionally, since insufficient input data may lead the encoder to learn collapsed embeddings, these CL methods expect a relatively large number of training data (e.g., large batch size or memory bank) to contrast. However, not all contrastive pairs are always informative and discriminative enough for the training processing. Therefore, a more general CL-based recommendation model called Meta-optimized Contrastive Learning for sequential Recommendation (MCLRec) is proposed in this work. By apply- ing both data augmentation and learnable model augmentation operations, this work innovates the standard CL framework by con- trasting data and model augmented views for adaptively capturing the informative features hidden in stochastic data augmentation. Moreover, MCLRec utilizes a meta-learning manner to guide the updating of the model augmenters, which helps to improve the quality of contrastive pairs without enlarging the amount of in- put data. Finally, a contrastive regularization term is considered to encourage the augmentation model to generate more informative augmented views and avoid too similar contrastive pairs within the meta updating. The experimental results on commonly used datasets validate the effectiveness of MCLRec.